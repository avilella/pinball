
=pod 

=head1 NAME

    Pinball::PipeConfig::Pinball_conf;

=head1 SYNOPSIS

   # Example 1: specifying only the mandatory option (numbers to be multiplied are taken from defaults)
init_pipeline.pl Bio::EnsEMBL::Hive::PipeConfig::LongMult_conf -password <mypass>

   # Example 2: specifying the mandatory options as well as overriding the default numbers to be multiplied:
init_pipeline.pl Bio::EnsEMBL::Hive::PipeConfig::LongMult_conf -password <mypass> -first_mult 2344556 -second_mult 777666555

   # Example 3: do not re-create the database, just load another multiplicaton task into an existing one:
init_pipeline.pl Bio::EnsEMBL::Hive::PipeConfig::LongMult_conf -job_topup -password <mypass> -first_mult 1111222233334444 -second_mult 38578377835


=head1 DESCRIPTION

    This is the PipeConfig file for the long pinball pipeline.

    Please see the implementation details in Runnable modules themselves.

=head1 CONTACT

    Please contact avlella@gmail.com mailing list with questions/suggestions.

=cut


package Pinball::PipeConfig::Pinball_conf;

use strict;
use warnings;
use Cwd;

use base ('Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf');  # All Hive databases configuration files should inherit from HiveGeneric, directly or indirectly


=head2 default_options

    Description : Implements default_options() interface method of Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf that is used to initialize default options.
                  In addition to the standard things it defines two options, 'first_mult' and 'second_mult' that are supposed to contain the long numbers to be multiplied.

=cut

sub default_options {
    my ($self) = @_;

    return {
        'ensembl_cvs_root_dir' => $ENV{'ENSEMBL_CVS_ROOT_DIR'},     # it will make sense to set this variable if you are going to use ehive frequently

        'pipeline_name' => 'pinball',                     # name used by the beekeeper to prefix job names on the farm

        'disk'           => 1000000,
        'cluster_threads'=> 1,
        'overlap'        => 31,
        'csize'          => 50,
        'erate'          => 0,
        'minreadlen'     => 34,
        'dust'           => 1,
        'sga_executable' => $ENV{'HOME'}.'/pinball/sga/src/sga',
        'work_dir'       => $ENV{'HOME'}.'/pinball_workdir',
        'phred64'        => 0,
        'tag'            => '',
        'no_permute'     => '',
        'sample'         => '',
        'minclustersize' => 10,
        'maxclustersize' => 100000,
        'email'          => 'avilella@gmail.com',           # for automatic notifications (may be unsupported

        'pipeline_db' => {                                  # connection parameters
            -host   => 'mysql-pinball',
            -port   => 4307,
            -user   => 'admin',
            -pass   => $self->o('password'),                        # a rule where a previously undefined parameter is used (which makes either of them obligatory)
            -dbname => $ENV{USER}.'_'.$self->o('pipeline_name'),    # a rule where a previously defined parameter is used (which makes both of them optional)
        },
    };
}

sub pipeline_wide_parameters {  # these parameter values are visible to all analyses, can be overridden by parameters{} and input_id{}
    my ($self) = @_;
    return {
        %{$self->SUPER::pipeline_wide_parameters},          # here we inherit anything from the base class
            'work_dir'       => $self->o('work_dir'),
            'overlap'        => $self->o('overlap'),
            'csize'          => $self->o('csize'),
            'erate'          => $self->o('erate'),
            'minreadlen'     => $self->o('minreadlen'),
            'dust'           => $self->o('dust'),
            'sga_executable' => $self->o('sga_executable'),
            'phred64'        => $self->o('phred64'),
            'tag'            => $self->o('tag'),
            'no_permute'     => $self->o('no_permute'),
            'sample'         => $self->o('sample'),
            'minclustersize' => $self->o('minclustersize'),
            'maxclustersize' => $self->o('maxclustersize'),
            'email'          => $self->o('email'),
    };
}


=head2 pipeline_create_commands

    Description : Implements pipeline_create_commands() interface method of Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf that lists the commands that will create and set up the Hive database.
                  In addition to the standard creation of the database and populating it with Hive tables and procedures it also creates two pipeline-specific tables used by Runnables to communicate.

=cut

# sub pipeline_create_commands {
#     my ($self) = @_;
#     return [
#         @{$self->SUPER::pipeline_create_commands},  # inheriting database and hive tables' creation
#            ];
# }


sub resource_classes {
    my ($self) = @_;
    return {
         0 => { -desc => 'default',          'LSF' => '' },
         1 => { -desc => 'cluster_req',      'LSF' => '-C0 -M'.$self->o('cluster_gigs').'000 -n '.$self->o('cluster_threads').' -q '.$self->o('cluster_queue').' -R"select[ncpus>='.$self->o('cluster_threads').' && mem>'.$self->o('cluster_gigs').'000] rusage[mem='.$self->o('cluster_gigs').'000] span[hosts=1]"' },
#        2 => { -desc => 'db_example',      'LSF' => '-R"select['.$self->o('dbresource').'<'.$self->o('dbserver_capacity').'] rusage['.$self->o('dbresource').'=10:duration=10:decay=1]"' },
    };
}


=head2 pipeline_analyses

    Description : Implements pipeline_analyses() interface method of Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf that defines the structure of the pipeline: analyses, jobs, rules, etc.
                  Here it defines three analyses:

                    * 'start' with two jobs (multiply 'first_mult' by 'second_mult' and vice versa - to check the commutativity of multiplivation).
                      Each job will dataflow (create more jobs) via branch #2 into 'part_multiply' and via branch #1 into 'add_together'.

                    * 'part_multiply' initially without jobs (they will flow from 'start')

                    * 'add_together' initially without jobs (they will flow from 'start').
                       All 'add_together' jobs will wait for completion of *all* 'part_multiply' jobs before their own execution (to ensure all data is available).

=cut

sub pipeline_analyses {
    my ($self) = @_;
    return [
        {   -logic_name => 'cluster',
            -module     => 'Pinball::Cluster',
            -parameters => { 
                            'readsfile'      => $self->o('readsfile'),
                            'cluster_threads' => $self->o('cluster_threads'),
                           },
            -input_ids  => [ {} ],    # only this job is needed at the beginning
            -flow_into => {
                2 => [ 'walk' ],      # will create a fan of jobs
                1 => [ 'report'  ],   # will create a funnel job to wait for the fan to complete and add the results
            },
            -rc_id => 1,
        },
        {   -logic_name => 'walk',
            -module     => 'Pinball::Walk',
            -parameters => {},
            -flow_into => {
                1 => [ 'blast'  ],
            },
            # input_id comes from cluster
        },
        {   -logic_name => 'blast',
            -module     => 'Pinball::Blast',
            -parameters => { 'readsfile' => $self->o('readsfile') },
        },
        {   -logic_name => 'report',
            -module     => 'Pinball::Report',
            -parameters => {},
            -wait_for => [ 'walk', 'blast' ],   # comes from the first analysis
        },
    ];
}

1;

